{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code was generated on Ubuntu 16.10 using python3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Venv and Imports](#chapter1)\n",
    "* [Transformers - Questions Generation](#chapter3)\n",
    "    * [Improvements](#chapter3.1)\n",
    "* [Speech2Text](#chapter4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venv and Imports <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "<h1> Create a virtual environment </h1>\n",
    "<h> Run this jupyter notebook in the virtual environment </h> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv linc_sayali\n",
    "! . linc_sayali/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install following packages, if not available, in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/sayali/.local/lib/python3.6/site-packages (4.4.2)\n",
      "Requirement already satisfied: packaging in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: importlib-metadata in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: dataclasses in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: filelock in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (2.0.6)\n",
      "Requirement already satisfied: requests in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sayali/.local/lib/python3.6/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/sayali/.local/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sayali/.local/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/sayali/.local/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sayali/.local/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.22)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: click in /home/sayali/.local/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /home/sayali/.local/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/sayali/.local/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/sayali/.local/lib/python3.6/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sayali/.local/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/sayali/.local/lib/python3.6/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /home/sayali/.local/lib/python3.6/site-packages (from torch) (0.8)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nlp in /home/sayali/.local/lib/python3.6/site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sayali/.local/lib/python3.6/site-packages (from nlp) (4.59.0)\n",
      "Requirement already satisfied: numpy in /home/sayali/.local/lib/python3.6/site-packages (from nlp) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /home/sayali/.local/lib/python3.6/site-packages (from nlp) (0.8)\n",
      "Requirement already satisfied: filelock in /home/sayali/.local/lib/python3.6/site-packages (from nlp) (2.0.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/sayali/.local/lib/python3.6/site-packages (from nlp) (2.25.1)\n",
      "Requirement already satisfied: dill in /home/sayali/.local/lib/python3.6/site-packages (from nlp) (0.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/sayali/.local/lib/python3.6/site-packages (from nlp) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sayali/.local/lib/python3.6/site-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->nlp) (1.22)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->nlp) (2.6)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/sayali/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /home/sayali/.local/lib/python3.6/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in /home/sayali/.local/lib/python3.6/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: regex in /home/sayali/.local/lib/python3.6/site-packages (from nltk) (2021.3.17)\n",
      "Requirement already satisfied: click in /home/sayali/.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: SpeechRecognition in /home/sayali/.local/lib/python3.6/site-packages (3.8.1)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/sayali/.local/lib/python3.6/site-packages (0.1.95)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow_hub in /home/sayali/.local/lib/python3.6/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/sayali/.local/lib/python3.6/site-packages (from tensorflow_hub) (1.19.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/sayali/.local/lib/python3.6/site-packages (from tensorflow_hub) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/sayali/.local/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade transformers\n",
    "!pip3 install torch\n",
    "!pip3 install nlp\n",
    "!pip3 install nltk\n",
    "!pip3 install SpeechRecognition\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If following imports don't work, then try to restart the notebook kernel. Sometimes, the installed packages are not reflected in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import matplotlib\n",
    "import nltk\n",
    "import nlp\n",
    "import torch\n",
    "import transformers \n",
    "import speech_recognition as sr\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - Questions Generation <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoQAGeneration:\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def read_data(self):\n",
    "        \"\"\"\n",
    "        Renaming the columns to human readable format and used as inputs in the transformers.\n",
    "        \"\"\"\n",
    "        rename_columns = {\n",
    "            'product_id': 'product id',\n",
    "            'category_name': 'category',\n",
    "            'price': 'price',\n",
    "            'rating': 'rating',\n",
    "            'name': 'name',\n",
    "            'attribute.fittext': 'fit measurements',\n",
    "            'attribute.colour': 'color',\n",
    "            'color_group': 'color group',\n",
    "            'attribute.size': 'size',\n",
    "            'attribute.gender': 'gender',\n",
    "            'attribute.style': 'style',\n",
    "            'attribute.itemtype': 'type',\n",
    "            'attribute.material': 'material',\n",
    "            'google_product_category': 'google product category',\n",
    "            'condition': 'condition',\n",
    "            'shop_info': 'shop information',\n",
    "            'age_group': 'age group',\n",
    "            'sku_code': 'code',\n",
    "            'department': 'department',\n",
    "            'fitname': 'fit name',\n",
    "            'fit': 'fit',\n",
    "            'attribute.description': 'description'\n",
    "        }\n",
    "        data = pd.read_csv(self.data_path, sep='\\t')\n",
    "        data = data.rename(rename_columns, axis=1)\n",
    "        return data\n",
    "        \n",
    "    def get_question(self, answer, context, tokenizer, model, max_length=128):\n",
    "        \"\"\"\n",
    "        This function takes answer and context as inputs and outputs a question generated from the context and oriented \n",
    "        toward the answer.\n",
    "        :param answer: context is the text from which a question is derived\n",
    "        :param context: answer is used to generate a focussed question\n",
    "        :param max_length: max length of question\n",
    "        :return: generated question\n",
    "        \"\"\"\n",
    "        input_text = \"answer: %s  context: %s </s>\" % (answer, context) # Formatting input for T5\n",
    "        features = tokenizer([input_text], return_tensors='pt') # Creating inputs and attention mask using pretrained tokenizer\n",
    "\n",
    "        output = model.generate(input_ids=features['input_ids'], \n",
    "                   attention_mask=features['attention_mask'],\n",
    "                   max_length=max_length)\n",
    "        return tokenizer.decode(output[0])\n",
    "    \n",
    "    def print_to_file(self, PATH, dict_object, keys, values):\n",
    "        # create or open a file with append mode\n",
    "        f = open(PATH, \"a\")\n",
    "        f.write('[')\n",
    "        count = 0\n",
    "        for k,v in dict_object.items():\n",
    "            count = count + 1\n",
    "            # create each json object to be inserted in the file\n",
    "            temp_dict = {}\n",
    "            temp_dict[keys] = k\n",
    "            temp_dict[values] = list(v) # convert set to list for json formatting\n",
    "            json_object = json.dumps(temp_dict, indent = 4) # dump json on the file\n",
    "\n",
    "            f.write(json_object) # write json object on file\n",
    "            if count < len(dict_object):\n",
    "                f.write(\",\")\n",
    "                f.write('\\n')\n",
    "        f.write(']')\n",
    "\n",
    "        f.close() # close the file\n",
    "    \n",
    "    def create_answers(self, write_to_file=False):\n",
    "        aq_dict = {}\n",
    "        qa_dict = {}\n",
    "        \n",
    "        data = self.read_data()\n",
    "        \n",
    "        \"\"\"\n",
    "        Answers are generated as the value of every column in each row. For that answer, create a context and generate a question. \n",
    "        This process will repeat for each value in the dataframe. \n",
    "        Time complexity = O(n*m)\n",
    "        n = num of rows\n",
    "        m = num of columns\n",
    "        \"\"\"\n",
    "        # Using T5 transformer model for text generation. Using the pretrained models to generate questions given the context\n",
    "        # and answer.\n",
    "\n",
    "        # AutoTokenizer is called to tokenize the inputs for t5 model. AutoModelWithLMHead is multi-head attention model\n",
    "        # and called to predict a question. \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "        model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "        \n",
    "        for num in range(len(data)):\n",
    "            # Keep each row as dataframe, so to use the column names in the context formation\n",
    "            curr_data = data.iloc[num]\n",
    "            if num%5==0:\n",
    "                print(\"loop # \", num, \"/\", str(len(data)))\n",
    "            for i in data.columns:\n",
    "                # create context and answer\n",
    "                # keeping the context short, so that questions are particular and focussed to that answer \n",
    "                context = str(curr_data['gender']) + \" \" + str(curr_data['type']) + \" \" + i + \" \" + str(curr_data[i])\n",
    "                answer = str(curr_data[i])\n",
    "\n",
    "                # generate question using T5 model\n",
    "                question = self.get_question(answer, context, tokenizer, model).replace(\"<pad> question: \", \"\").replace('</s>', '')\n",
    "\n",
    "                # create answer-question dictionery to put in file\n",
    "                # key = answer, value = questions\n",
    "                if answer in aq_dict:\n",
    "                    aq_dict[answer].add(question)\n",
    "                else:\n",
    "                    aq_dict[answer] = {question}\n",
    "\n",
    "                # create question-answer dictionery to put it in file, later used for question matching\n",
    "                # key = question, value = answers\n",
    "                if question in qa_dict:\n",
    "                    qa_dict[question].add(answer)\n",
    "                else:\n",
    "                    qa_dict[question] = {answer}\n",
    "                      \n",
    "        if write_to_file:\n",
    "            # autogenerated_QA.txt is created and is used for next task\n",
    "            # This filename is used to test out the output from code to file\n",
    "            PATH = \"autogenerated_QA_1.txt\"\n",
    "            self.print_to_file(PATH, dict_object=aq_dict, keys='answer', values='questions')\n",
    "            \n",
    "            PATH = \"autogenerated_questions_1.txt\"\n",
    "            self.print_to_file(PATH, dict_object=qa_dict, keys='question', values='answers')\n",
    "                      \n",
    "        return qa_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "/home/sayali/.local/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:1010: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop #  0 / 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop #  5 / 78\n",
      "loop #  10 / 78\n",
      "loop #  15 / 78\n",
      "loop #  20 / 78\n",
      "loop #  25 / 78\n",
      "loop #  30 / 78\n",
      "loop #  35 / 78\n",
      "loop #  40 / 78\n",
      "loop #  45 / 78\n",
      "loop #  50 / 78\n",
      "loop #  55 / 78\n",
      "loop #  60 / 78\n",
      "loop #  65 / 78\n",
      "loop #  70 / 78\n",
      "loop #  75 / 78\n"
     ]
    }
   ],
   "source": [
    "qa_gen = AutoQAGeneration('product_catalog.tsv')\n",
    "qa_dict = qa_gen.create_answers(write_to_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "Generated data was manually checked to see if the questions and answers are correct as of catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements <a class=\"anchor\" id=\"chapter3.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question generation task can be improved by a few methods mentioned below:\n",
    "\n",
    "- Training the T5 model for question generation and fine tune it to our needs. Reason I didn't train the model is we are using generic words, in our dataset, on which T5 models are already pretrained. \n",
    "\n",
    "- Creating carefully curated context, to generate wide variety of questions. This can be done by the method I mentioned in my code, where I am combining different columns along with column names to generate a short and concise context for question to be generated. For short context, to generate different types of questions for one given answer is still a challenge since English language can be modified in many artistic ways. \n",
    "\n",
    "- Automatic answer generation. E.g. follow the code below. It takes in the context and generates answers automatically without any given questions. This is useful to generate the question. when I used this method, it didn't provide the desired answers. Further it could improved by training the model with our dataset and try to generate the answers. These answers could be used to generate questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model_qa = AutoModelForSeq2SeqLM.from_pretrained(\"valhalla/t5-small-qa-qg-hl\")\n",
    "\n",
    "def get_question_answer(context, max_length=128):\n",
    "  input_text = \"context: %s </s>\" % (context)\n",
    "  features = tokenizer([input_text], return_tensors='pt')\n",
    "  \n",
    "  output = model_qa.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length)\n",
    "\n",
    "  return tokenizer.decode(output[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use similar words in order to increase the questions set. E.g. jeans category is bottoms, it could also be called pants, men are also called males etc.\n",
    "\n",
    "- Build a text generator to automatically generate query from a question for finding a better answer. E.g. Are there jeans available in women's department below \\$50? It should generate SQL like query which can answer that question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference links\n",
    "### Code\n",
    "- https://github.com/huggingface/transformers\n",
    "\n",
    "### Theory\n",
    "- https://www.coursera.org/learn/attention-models-in-nlp/home/welcome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech2Text <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps followed\n",
    "- First the utterances are recorded into .flac format. \n",
    "- Then converted into strings. \n",
    "- Pretrained models are used to create word embeddings for utterances and questions in the database. \n",
    "- The utterance and questions are matched based on their similarity score (cosine similary). \n",
    "- Top 3 answers are returned as per the utterance matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech2Text:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def get_audio_files(self, directory, audio_format=\".flac\"):\n",
    "        \"\"\"\n",
    "        This function reads audio files from the mentioned directory. \n",
    "        :param directory: Directory of the audio files\n",
    "        :param audio_format: Audio files to look for in the directory\n",
    "        :return: List of audio file paths\n",
    "        \"\"\"\n",
    "\n",
    "        import os\n",
    "\n",
    "        audio_files = []\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.path.endswith(audio_format) and entry.is_file():\n",
    "                audio_files.append(entry.path)\n",
    "        return audio_files\n",
    "\n",
    "    def read_audio_files(self, file_path):\n",
    "        \"\"\"\n",
    "        This function reads the audio file and returns the utterance as string.\n",
    "        :param file_path: Audio file path\n",
    "        :return: Utterance as string\n",
    "        \"\"\"\n",
    "        # Set speech recognizer\n",
    "        r = sr.Recognizer()\n",
    "        \n",
    "        intro = sr.AudioFile(file_path)\n",
    "        with intro as source:\n",
    "            audio = r.record(source)\n",
    "        return r.recognize_google(audio)\n",
    "\n",
    "    def match_questions(self, utterance, sentences, sentence_embeddings, query_vec):\n",
    "        \"\"\"\n",
    "        This function takes the utterance and matches with the questions using cosine similarityand returns top \n",
    "        3 matched questions. \n",
    "        :param utterance: Input from audio file\n",
    "        :param sentences: List of questions as string\n",
    "        :param sentence_embeddings: Questions in embeddings format\n",
    "        :return: Top 3 questions matched with utterance\n",
    "        \"\"\"\n",
    "        d = {}\n",
    "        \n",
    "        for sent, sent_emb in zip(sentences, sentence_embeddings):\n",
    "            sim = cosine(query_vec, sent_emb)\n",
    "            # Only considering the match with similarity score 50%\n",
    "            if sim < 0.5:\n",
    "                d[sim] = sent\n",
    "\n",
    "        if d: \n",
    "            d = dict(sorted(d.items(), key=lambda item: item[0]))\n",
    "            if len(d) >= 3:\n",
    "                return list(d.values())[0:3]\n",
    "            else:\n",
    "                return list(d.values())\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    def get_utterances(self, directory, print_output=False):\n",
    "        \"\"\"\n",
    "        This function takes the directory and returns utterances in string format. \n",
    "        :param directory: Directory where audio files reside\n",
    "        :param print_output: Print output of utterances as a proof\n",
    "        :return: Utterances as list of string\n",
    "        \"\"\"\n",
    "        # get all audio files\n",
    "        audio_files = self.get_audio_files(directory)\n",
    "\n",
    "        # reading all audio files and getting their utterances in string format\n",
    "        utterances = []\n",
    "        for file in audio_files:\n",
    "            utterances.append(self.read_audio_files(file))\n",
    "        if print_output:\n",
    "            print(utterances)\n",
    "            \n",
    "        return utterances\n",
    "    \n",
    "    def get_answers(self, utterances, sentences, sentence_embeddings, qa_dict, model_emb):\n",
    "        \"\"\"\n",
    "        This function takes in the utterances from audio files as query, and matches with questions from database. \n",
    "        Use model_emb to create word embeddings for every utterance. For every output of list of questions, return\n",
    "        top 3 answers. If the questions don't match or are irrelevant, then return NA. \n",
    "        :param utterances: audio file inputs\n",
    "        :param sentences: list of questions\n",
    "        :param sentence_embeddings: embeddings of questions\n",
    "        :param model_emb: model for word embedding\n",
    "        \"\"\"\n",
    "\n",
    "        # Matching questions for each utterance and returning answers\n",
    "        for utterance in utterances:\n",
    "            # Start tracking time\n",
    "            tic = time.time()\n",
    "\n",
    "            print('utterance =', utterance)\n",
    "            query_vec = model_emb([utterance])[0]\n",
    "\n",
    "            # Get matching questions for the utterance\n",
    "            questions = self.match_questions(utterance, sentences, sentence_embeddings, query_vec)\n",
    "            \n",
    "            if questions:\n",
    "                # Get top 3 answers for the utterance\n",
    "                answers = []\n",
    "                for q in questions:\n",
    "                    # find the answers from the dictionery created\n",
    "                    answers = answers + list(qa_dict[q])\n",
    "                if len(answers) >= 3:\n",
    "                    print(\"Top 3 answers to the utterances are\\n\", answers[0:3])\n",
    "                else:\n",
    "                    print(\"Top 3 answers to the utterances are\\n\", answers)\n",
    "            else:\n",
    "                print(\"NA\")\n",
    "\n",
    "            # End tracking time\n",
    "            tac = time.time()\n",
    "            print(\"Time taken for matching one utterance is\", (tac-tic)*1000, \"ms\")\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('autogenerated_questions.txt') as json_file:\n",
    "    datafile = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_input = {}\n",
    "for i in datafile:\n",
    "    key = i['question']\n",
    "    value = i['answers']\n",
    "    if key in qa_input:\n",
    "        qa_input[key] = qa_input[key] + value\n",
    "    else:\n",
    "        qa_input[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance = what is the price of men's jeans\n",
      "Top 3 answers to the utterances are\n",
      " ['89.5', '69.5', '34.98']\n",
      "Time taken for matching one utterance is 24.939537048339844 ms\n",
      "\n",
      "\n",
      "utterance = what is the color of women's jeans\n",
      "Top 3 answers to the utterances are\n",
      " ['Black Squared', 'Medium Wash', 'Lonesome Road']\n",
      "Time taken for matching one utterance is 15.981435775756836 ms\n",
      "\n",
      "\n",
      "utterance = what are women's jeans measurement\n",
      "Top 3 answers to the utterances are\n",
      " ['Skinny, Super Skinny', 'Mid rise: Sits at waist, Relaxed through hip and thigh, Straight leg', '23x30']\n",
      "Time taken for matching one utterance is 8.982658386230469 ms\n",
      "\n",
      "\n",
      "utterance = what is the condition of men's jeans\n",
      "Top 3 answers to the utterances are\n",
      " ['NEW', 'NEW', '100% Cotton, 5-pocket styling, Button Fly, Imported, Read our definitive guide to making jeans last longer <strong><a href=\"https://www.levi.com/US/en_US/blog/article/the-definitive-guide-to-denim/\" target=\"_blank\">here</a></strong>, Wash your jeans once every 10 wears at most; this increases their lifespan and saves natural resources, When you eventually launder your jeans, wash and dry them inside out with like colors; liquid detergent is recommended']\n",
      "Time taken for matching one utterance is 6.426572799682617 ms\n",
      "\n",
      "\n",
      "utterance = what is the rating of men's jeans\n",
      "Top 3 answers to the utterances are\n",
      " ['4.3863', '4.1223', '3.8636']\n",
      "Time taken for matching one utterance is 5.496978759765625 ms\n",
      "\n",
      "\n",
      "utterance = what is the condition of women's jeans\n",
      "Top 3 answers to the utterances are\n",
      " ['NEW', 'NEW', '69.5']\n",
      "Time taken for matching one utterance is 17.58098602294922 ms\n",
      "\n",
      "\n",
      "utterance = what are men's jeans measurement\n",
      "Top 3 answers to the utterances are\n",
      " ['Measurements are taken from a size 33 waist, Regular fit through the thigh, Sits at your waist; Front rise: 11.25\", Straight leg opening: 16\"', 'Skinny, Super Skinny', 'Mid rise: Sits at waist, Relaxed through hip and thigh, Straight leg']\n",
      "Time taken for matching one utterance is 16.963481903076172 ms\n",
      "\n",
      "\n",
      "utterance = what is the price of women's jeans\n",
      "Top 3 answers to the utterances are\n",
      " ['69.5', '34.98', '89.5']\n",
      "Time taken for matching one utterance is 9.663105010986328 ms\n",
      "\n",
      "\n",
      "utterance = what is the color of mainstream\n",
      "Top 3 answers to the utterances are\n",
      " ['Black Squared', 'Medium Wash', 'Lonesome Road']\n",
      "Time taken for matching one utterance is 6.69097900390625 ms\n",
      "\n",
      "\n",
      "utterance = hello my name is Riley\n",
      "NA\n",
      "Time taken for matching one utterance is 15.766620635986328 ms\n",
      "\n",
      "\n",
      "utterance = what is the rating of women's jeans\n",
      "Top 3 answers to the utterances are\n",
      " ['4.1223', '3.8636', '4.3863']\n",
      "Time taken for matching one utterance is 10.11967658996582 ms\n",
      "\n",
      "\n",
      "utterance = what is the size of women's pants\n",
      "Top 3 answers to the utterances are\n",
      " ['23x30', '27x30', '24x30']\n",
      "Time taken for matching one utterance is 6.349802017211914 ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained TFhub model for word embeddings\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model_emb = hub.load(module_url)\n",
    "\n",
    "# List questions generated in the previous task\n",
    "sentences = np.array(list(qa_input.keys()))\n",
    "\n",
    "# Get embeddings for questions using above model\n",
    "sentence_embeddings = np.array(model_emb(sentences))\n",
    "\n",
    "s2t = Speech2Text()\n",
    "utterances = s2t.get_utterances(directory=\"./audio/\")\n",
    "\n",
    "s2t.get_answers(utterances, sentences, sentence_embeddings, qa_input, model_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is question a good match with utterance\n",
    "\n",
    "Above are 12 examples of QA. 1 of them is irrelavant and 11 of them are relevant to catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match utterance with questions, I used cosine similarity score on word embeddings. The method is fast and gives similar questions. For the question 'hello my name is Riley', no similar question was found and NA was returned. The similarity score ranges from 0 to 1. Values closer to 0 are more similar vectors and values close to 1 are different vectors. The threshold of 50% is used in order to find the similar questions. I.e. when the score is below 0.5, then the question is acceptable to the utterance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utterance 'what is the color of mainstream' was by mistakenly read 'mainstream' instead of 'men's jeans'. But the similarity score gave the color of the jeans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- https://realpython.com/python-speech-recognition/#picking-a-python-speech-recognition-package\n",
    "- https://tfhub.dev/google/universal-sentence-encoder/4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
